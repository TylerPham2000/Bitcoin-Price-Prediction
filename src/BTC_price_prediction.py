# -*- coding: utf-8 -*-
"""Predicting Price Of BTC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rpimp4q9hgeNeKHWIoJnyqVLMpLaI3yp
"""

# Predict the price of BTC

# Install neccessary dependencies
# pip install vaderSentiment
# pip install yfinance

# Google colab dependencies
from google.colab import files
from google.colab import drive

# Import all packages
import tweepy
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import datetime
import yfinance as yf
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Specify plot styles
plt.style.use('fivethirtyeight')

# Twitter APi credentials
consumerKey = ""
consumerSecret = ""
accessToken = ""
accessTokenSecret = ""

# Create authentication object
authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret)
# Get access tokens
authenticate.set_access_token(accessToken, accessTokenSecret)
# Create API object
api = tweepy.API(authenticate, wait_on_rate_limit=True)

# Tweet search criteria
# Only able to pull tweets less than 7 days old
search_term = '#bitcoin -filter:retweets'

# Create cursor object
tweets_list = tweepy.Cursor(api.search, q=search_term, lang= 'en', tweet_mode= 'extended').items(1000)

# Create list to store output
output = []

# Collect particular tweet credentials
for tweet in tweets_list:
    text = tweet._json["full_text"]
    favorite_count = tweet.favorite_count
    retweet_count = tweet.retweet_count
    created_at = tweet.created_at
    author = tweet.user.name
    twitter_name = tweet.user.screen_name

    # Format DataFrame
    line = {'date' : created_at, 'author' : author, 'twitter_user_name': twitter_name, 'like_count' : favorite_count, 'retweet_count' : retweet_count, 'tweet' : text}
    output.append(line)

# Create DataFrame = Output
df1 = pd.DataFrame(output)

# Display df1
df1

# Reformat "created_at" format
df1['date'] = pd.to_datetime(df1['date']).dt.normalize()

# Function that cleans the Tweets for sentiment analysis
def CleanTweet(twt):
  # Removes "#" from #Bitcoin
  twt = re.sub('#Bitcoin', 'Bitcoin', twt)
  # Removes "#" from #bitcoin
  twt = re.sub('#bitcoin', 'bitcoin', twt)
  # Remove the hashtags, letters, and numbers from a Tweet
  twt = re.sub('#[A-Za-z0-9]+', '', twt)
  # Remove any spaces within a Tweet
  twt = re.sub('\\n', '', twt)
  # Remove any links contained in a Tweet
  twt = re.sub('https?:\/\/\S+', '', twt)
  # Return twt
  return twt

# Apply CleanTweet function to DataFrame
df1['Cleaned_Tweets'] = df1['tweet'].apply(CleanTweet)

# Create a function to generate the polarity of each Tweet
def getPolarity(twt):
  return TextBlob(twt).sentiment.polarity
# Create a function to generate the subjectivity of each Tweet
def getSubjectivity(twt):
  return TextBlob(twt).sentiment.subjectivity
# Apply polarity rating to Cleaned_Tweets
df1['Polarity'] = df1['Cleaned_Tweets'].apply(getPolarity)
# Apply subjectivity rating to Cleaned_Tweets
df1['Subjectivity'] = df1['Cleaned_Tweets'].apply(getSubjectivity)

# Produce sentiment rating for each Tweet
def getSentiment(score):
  if score < 0:
    return 'Negative'
  elif score == 0:
    return 'Neutral'
  else:
    return 'Positive'

# Add sentiment rating to the DataFrame
df1['Sentiment'] = df1['Polarity'].apply(getSentiment)

# Add compound scores to DataFrame
analyzer = SentimentIntensityAnalyzer()
df1['neg'] = [analyzer.polarity_scores(x)['neg'] for x in df1['Cleaned_Tweets']]
df1['neu'] = [analyzer.polarity_scores(x)['neu'] for x in df1['Cleaned_Tweets']]
df1['pos'] = [analyzer.polarity_scores(x)['pos'] for x in df1['Cleaned_Tweets']]
df1['compound'] = [analyzer.polarity_scores(x)['compound'] for x in df1['Cleaned_Tweets']]

# Display Output DataFrame
df1.head()

# Display data visually
plt.title('Bitcoin Sentiment Analysis')
df1['Sentiment'].value_counts().plot(kind = 'bar')
plt.xlabel('Sentiments')
plt.ylabel('Number of Counts')
plt.show()

# Calculate average polarity by day
polarity = df1.groupby(['date']).sum()['Polarity']
polarity_count = df1.groupby(['date']).count()['Polarity']
average_polarity = polarity / polarity_count
print(f"Average polarity rating for {average_polarity}")

# Calculate average compound score by day
compound_score = df1.groupby(['date']).sum()['compound']
compound_count = df1.groupby(['date']).count()['compound']
average_compound = compound_score / compound_count
print(f"Average compund rating for {average_compound}")

# Collect Bitcoin data

# Create empty list
output2 = []

# Specify which asset
btc = yf.Ticker("BTC-USD")
print(btc)

# Get Bitcoin info
btc.info

# Determine dates collected
data = btc.history(period="max")

# Place data into DataFrame
df2 = pd.DataFrame(data)

# Format DataFrame
df2.drop(df2.columns[[5, 6]], axis = 1, inplace = True)

df2

df1

# Create variable for predicting price 'n' days into the future
projection = 4
# Create new column named prediction
df2['Prediction'] = df2[['Close']].shift(-projection)
# Display the data
df2

# Create the independent data set (X)
X = np.array (df2[['Close']])
# Remove the last 4 rows
X = X[:-projection]
# print 'X'
print(X)

# Create the dependent data set (Y)
y = df2['Prediction'].values

y = y[:-projection]

print(y)

# Split data into training and testing data sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = .20)

# Create and train model
linReg = LinearRegression()
# Train the model
linReg.fit(x_train, y_train)

# Test the model using R squared
linReg_confidence = linReg.score(x_test, y_test)
# Print score
print('Linear regression confidence:', linReg_confidence)

# Create new variable: x_projection and set it to last 4 rows of original data set
x_projection = np.array(df2[['Close']])[-projection:]

print(x_projection)

# Print the linear regression model prediction for the next 4 days
linReg_prediction = linReg.predict(x_projection)

print(f"Price prediction of Bitcoin for the next 4 days:{linReg_prediction}")
